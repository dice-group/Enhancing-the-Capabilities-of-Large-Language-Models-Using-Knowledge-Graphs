{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "import ast\n",
    "import evaluate\n",
    "\n",
    "def load_config():\n",
    "    try:\n",
    "        with open(r\"config.json\") as f:\n",
    "            return json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Config file not found. Please check the path.\")\n",
    "\n",
    "def initialize_azure_client(config):\n",
    "    client = SecretClient(vault_url=config.key_vault_url, credential=DefaultAzureCredential())\n",
    "    secret = client.get_secret(config.dev_secret_name)\n",
    "    return AzureOpenAI(api_key=secret.value, api_version=config.chat.api_version, azure_endpoint=config.chat.azure_endpoint)\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "config = load_config()\n",
    "llm = initialize_azure_client(config)\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_to_set(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_urls(text):\n",
    "        \"\"\" Removes all URLs starting with 'http://' or 'https://' \"\"\"\n",
    "        return re.sub(r'http[s]?://\\S+', '', text)\n",
    "    \n",
    "    if not isinstance(s, str):\n",
    "        s = str(s) if s else \"\"\n",
    "\n",
    "    text = lower(s)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punc(text)\n",
    "    text = remove_articles(text)\n",
    "    text = white_space_fix(text)\n",
    "\n",
    "    tokens = text.split()\n",
    "    return set(tokens)\n",
    "\n",
    "def process_retrieved_items(retrieved_list):\n",
    "    \"\"\"\n",
    "    Convert the 'retrieved result' field into a list of strings\n",
    "    excluding 'sources'.\n",
    "    \"\"\"\n",
    "    result_strings = []\n",
    "    for item in retrieved_list:\n",
    "        for key, val in item.items():\n",
    "            if key.lower() != \"sources\":\n",
    "                result_strings.append(str(val))\n",
    "    return result_strings\n",
    "\n",
    "# Function to calculate comprehensiveness using LLM\n",
    "\n",
    "def calculate_comprehensiveness(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of comprehensiveness.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    Comprehensiveness: \"How much detail does the answer provide to cover all the aspects and details of the\n",
    "    question? A comprehensive answer should be thorough and complete, without being redundant or irrelevant.\n",
    "    For example, if the question is 'What are the benefits and drawbacks of nuclear energy?', a comprehensive\n",
    "    answer would provide both the positive and negative aspects of nuclear energy, such as its efficiency,\n",
    "    environmental impact, safety, cost, etc. A comprehensive answer should not leave out any important points\n",
    "    or provide irrelevant information. For example, an incomplete answer would only provide the benefits of\n",
    "    nuclear energy without describing the drawbacks, or a redundant answer would repeat the same information\n",
    "    multiple times.\"\n",
    "\n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how comprehensive the response is. Where 0 is not comprehensive and 1 is the most comprehensive.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "\n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate relevance using LLM\n",
    "def calculate_relevance(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of relevance.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    Relevance: \"How well does the answer address the user query? A relevant answer should be directly related\n",
    "    to the question asked and provide information that is useful and on-topic. For example, if the question is\n",
    "    'What is the capital of France?', a relevant answer would be 'Paris'. A relevant answer should not provide\n",
    "    information that is unrelated to the question or that does not directly address the user's information need.\n",
    "    For example, an irrelevant answer would be 'The Eiffel Tower is located in Paris'.\"\n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how relevant the response is. Where 0 is not relevant and 1 is the most relevant.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "    \n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate directness using LLM\n",
    "def calculate_directness(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of directness.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    directness: \"How specifically and clearly does the answer address the question? A direct answer should\n",
    "    provide a clear and concise answer to the question. For example, if the question is 'What is the capital\n",
    "    of France?', a direct answer would be 'Paris'. A direct answer should not provide any irrelevant or\n",
    "    unnecessary information that does not answer the question. For example, an indirect answer would be 'The\n",
    "    capital of France is located on the river Seine'.\"\n",
    "    \n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how direct the response is. Where 0 is not direct and 1 is the most direct.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "\n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "    \n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "def separate_characters(line):\n",
    "    return list(line.strip().replace(\" \", \"\"))\n",
    "\n",
    "def separate_punctuation(line):\n",
    "    words = line.strip().split()\n",
    "    tokenized = []\n",
    "    for w in words:\n",
    "        if len(w) == 1:\n",
    "            tokenized.append(w)\n",
    "        else:\n",
    "            lastChar = w[-1] \n",
    "            firstChar = w[0]\n",
    "            if lastChar in string.punctuation:\n",
    "                tokenized += [w[:-1], lastChar]\n",
    "            elif firstChar in string.punctuation:\n",
    "                tokenized += [firstChar, w[1:]]\n",
    "            else:\n",
    "                tokenized.append(w)\n",
    "    return tokenized\n",
    "    \n",
    "def ngram_counts(wordList, order):\n",
    "    counts = defaultdict(lambda: defaultdict(float))\n",
    "    nWords = len(wordList)\n",
    "    for i in range(nWords):\n",
    "        for j in range(1, order+1):\n",
    "            if i+j <= nWords:\n",
    "                ngram = tuple(wordList[i:i+j])\n",
    "                counts[j-1][ngram]+=1\n",
    "    return counts\n",
    "\n",
    "def ngram_matches(ref_ngrams, hyp_ngrams):\n",
    "    matchingNgramCount = defaultdict(float)\n",
    "    totalRefNgramCount = defaultdict(float)\n",
    "    totalHypNgramCount = defaultdict(float)\n",
    "    for order in ref_ngrams:\n",
    "        for ngram in hyp_ngrams[order]:\n",
    "            totalHypNgramCount[order] += hyp_ngrams[order][ngram]\n",
    "        for ngram in ref_ngrams[order]:\n",
    "            totalRefNgramCount[order] += ref_ngrams[order][ngram]\n",
    "            if ngram in hyp_ngrams[order]:\n",
    "                matchingNgramCount[order] += min(ref_ngrams[order][ngram], hyp_ngrams[order][ngram])\n",
    "    return matchingNgramCount, totalRefNgramCount, totalHypNgramCount\n",
    "\n",
    "def ngram_precrecf(matching, reflen, hyplen, beta):\n",
    "    factor = beta**2\n",
    "    totalPrec = sum(matching.values()) / sum(hyplen.values()) if sum(hyplen.values()) else 1e-16\n",
    "    totalRec = sum(matching.values()) / sum(reflen.values()) if sum(reflen.values()) else 1e-16\n",
    "    denom = factor * totalPrec + totalRec\n",
    "    totalF = ((1+factor)*totalPrec*totalRec / denom) if denom > 0 else 1e-16\n",
    "    return totalF\n",
    "\n",
    "def calculate_chrF(reference, hypothesis, nworder=2, ncorder=6, beta=2.0):\n",
    "    ref_words = separate_punctuation(reference)\n",
    "    hyp_words = separate_punctuation(hypothesis)\n",
    "    ref_chars = separate_characters(reference)\n",
    "    hyp_chars = separate_characters(hypothesis)\n",
    "\n",
    "    ref_word_ngram = ngram_counts(ref_words, nworder)\n",
    "    hyp_word_ngram = ngram_counts(hyp_words, nworder)\n",
    "    ref_char_ngram = ngram_counts(ref_chars, ncorder)\n",
    "    hyp_char_ngram = ngram_counts(hyp_chars, ncorder)\n",
    "\n",
    "    matching_word, ref_word_total, hyp_word_total = ngram_matches(ref_word_ngram, hyp_word_ngram)\n",
    "    matching_char, ref_char_total, hyp_char_total = ngram_matches(ref_char_ngram, hyp_char_ngram)\n",
    "\n",
    "    word_fscore = ngram_precrecf(matching_word, ref_word_total, hyp_word_total, beta)\n",
    "    char_fscore = ngram_precrecf(matching_char, ref_char_total, hyp_char_total, beta)\n",
    "\n",
    "    # Average word and char f-scores\n",
    "    total_fscore = (word_fscore + char_fscore) / 2\n",
    "    return total_fscore\n",
    "\n",
    "def evaluate_dataset_row(gold_str, final_ans_str, retrieved_list):\n",
    "    \"\"\"\n",
    "    Compute metrics for a single query/row:\n",
    "    1) Accuracy: check if all gold tokens appear in final_ans_tokens\n",
    "    2) Precision/Recall for retrieved\n",
    "    3) F1 for retrieved\n",
    "    4) Hits@1: check if any retrieved item matches gold tokens\n",
    "    5) MRR: check if any retrieved item matches gold tokens\n",
    "    6) BLEU: check if any retrieved item matches gold tokens\n",
    "    7) ROUGE: check if any retrieved item matches gold tokens\n",
    "    8) chrF: check if any retrieved item matches gold tokens\n",
    "    9) Comprehensiveness: check if any retrieved item matches gold tokens\n",
    "    10) Relevance: check if any retrieved item matches gold tokens\n",
    "    11) Directness: check if any retrieved item matches gold tokens\n",
    "\n",
    "    Returns a dict of these metrics for that query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'retrieved_list' -> list of non-source strings\n",
    "    retrieved_clean = process_retrieved_items(retrieved_list)\n",
    "\n",
    "    # Normalize tokens\n",
    "    gold_tokens = normalize_to_set(gold_str)\n",
    "    final_ans_tokens = normalize_to_set(final_ans_str)\n",
    "\n",
    "    # 1) Accuracy: check if all gold tokens appear in final_ans_tokens\n",
    "    acc = 1.0 if gold_tokens.issubset(final_ans_tokens) else 0.0\n",
    "    print(\"accuracy\", acc)\n",
    "    # 2) Precision/Recall for retrieved\n",
    "    retrieved_str = \" \".join(retrieved_clean)\n",
    "    pred_set = normalize_to_set(retrieved_str)\n",
    "    intersection = pred_set.intersection(gold_tokens)\n",
    "    print(\"predicted\", pred_set)\n",
    "    print(\"gold\", gold_tokens)\n",
    "    print(\"intersection\", intersection)\n",
    "    num_common = len(intersection)\n",
    "    if len(pred_set) == 0 and len(gold_tokens) == 0:\n",
    "        p_ret, r_ret = 1.0, 1.0\n",
    "    else:\n",
    "        p_ret = num_common / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "        r_ret = num_common / len(gold_tokens) if len(gold_tokens) > 0 else 0.0\n",
    "    print(\"precision\", p_ret)\n",
    "    print(\"recall\", r_ret)\n",
    "    f1_score = 2 * (p_ret * r_ret) / (p_ret + r_ret) if (p_ret + r_ret) > 0 else 0.0\n",
    "    print(\"f1\", f1_score)\n",
    "    hits1 = 1.0 if num_common > 0 else 0.0\n",
    "    print(\"hits1\", hits1)\n",
    "    # 5) MRR: check if any retrieved item matches gold tokens\n",
    "    rank = None\n",
    "    for idx, candidate_str in enumerate(retrieved_clean):\n",
    "        cand_set = normalize_to_set(candidate_str)\n",
    "        if cand_set.intersection(gold_tokens):\n",
    "            rank = idx + 1\n",
    "            break\n",
    "\n",
    "    if rank is None:\n",
    "        mrr_val = 0.0\n",
    "\n",
    "    else:\n",
    "        mrr_val = 1.0 / rank\n",
    "    print(\"mrr\", mrr_val)   \n",
    "    predicted = str(final_ans_tokens).strip()\n",
    "    print(\"predicted string for blue and rouge\", predicted)\n",
    "    ground_truth = str(gold_tokens).strip()\n",
    "    print(\"ground truth string for blue and rouge\", ground_truth)\n",
    "    predicted_sentence = \" \".join(sorted(predicted))  # Convert back to sorted sentence\n",
    "    ground_truth_sentence = \" \".join(sorted(ground_truth))  # Convert back to sorted sentence\n",
    "\n",
    "    try:\n",
    "        bleu_result = bleu_metric.compute(predictions=[predicted_sentence], references=[[ground_truth_sentence]])\n",
    "        rouge_result = rouge_metric.compute(predictions=[predicted_sentence], references=[ground_truth_sentence])\n",
    "        bleu_score = bleu_result[\"bleu\"]\n",
    "        rouge_1 = rouge_result[\"rouge1\"]\n",
    "        rouge_2 = rouge_result[\"rouge2\"]\n",
    "        rouge_l = rouge_result[\"rougeL\"]\n",
    "        chrf_score = calculate_chrF(ground_truth_sentence, predicted_sentence) \n",
    "\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f\"Error computing BLEU for row : {e}\")\n",
    "        bleu_score = 0\n",
    "        rouge_1 = rouge_2 = rouge_l = chrf_score = 0\n",
    "    \n",
    "    print(\"bleu, rouge1, rouge2, rougeL\", bleu_score, rouge_1, rouge_2, rouge_l)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision(Retrieved)\": p_ret,\n",
    "        \"Recall(Retrieved)\": r_ret,\n",
    "        \"F1\": f1_score,\n",
    "        \"Hits\": hits1,\n",
    "        \"MRR\": mrr_val,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_1,\n",
    "        \"ROUGE-2\": rouge_2,\n",
    "        \"ROUGE-L\": rouge_l,\n",
    "        \"chrF\": chrf_score\n",
    "    }\n",
    "\n",
    "def evaluate_dataset_per_row(df):\n",
    "    \"\"\"\n",
    "    For each row in df, compute the row-level metrics and store them in new columns.\n",
    "    \"\"\"\n",
    "    # # Initialize empty columns\n",
    "    df[\"Accuracy\"] = 0.0\n",
    "    df[\"Precision(Retrieved)\"] = 0.0\n",
    "    df[\"Recall(Retrieved)\"] = 0.0\n",
    "    df[\"Hits\"] = 0.0\n",
    "    df[\"MRR\"] = 0.0\n",
    "    df[\"Comprehensiveness\"] = 0.0\n",
    "    df[\"Relevance\"] = 0.0   \n",
    "    df[\"Directness\"] = 0.0\n",
    "    df[\"BLEU Score\"] = 0.0\n",
    "    df[\"ROUGE-1\"] = 0.0\n",
    "    df[\"ROUGE-2\"] = 0.0\n",
    "    df[\"ROUGE-L\"] = 0.0\n",
    "    df[\"chrF\"] = 0.0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row[\"Query\"]\n",
    "        gold_str = row[\"Ground Truth Answer\"]\n",
    "        final_ans_str = row[\"LLM Answer\"]\n",
    "        retrieved_list = row[\"Blazegraph Response\"]\n",
    "\n",
    "        metrics_dict = evaluate_dataset_row(gold_str, final_ans_str, retrieved_list)\n",
    "        compre_score = calculate_comprehensiveness(query, final_ans_str)\n",
    "        rel_score = calculate_relevance(query, final_ans_str)\n",
    "        dir_score = calculate_directness(query, final_ans_str)\n",
    "        df.at[idx, \"Accuracy\"] = metrics_dict[\"Accuracy\"]\n",
    "        df.at[idx, \"Precision(Retrieved)\"] = metrics_dict[\"Precision(Retrieved)\"]\n",
    "        df.at[idx, \"Recall(Retrieved)\"] = metrics_dict[\"Recall(Retrieved)\"]\n",
    "        df.at[idx, \"Hits\"] = metrics_dict[\"Hits\"]\n",
    "        df.at[idx, \"MRR\"] = metrics_dict[\"MRR\"]\n",
    "        df.at[idx, \"BLEU Score\"] = metrics_dict[\"BLEU Score\"]\n",
    "        df.at[idx, \"ROUGE-1\"] = metrics_dict[\"ROUGE-1\"]\n",
    "        df.at[idx, \"ROUGE-2\"] = metrics_dict[\"ROUGE-2\"]\n",
    "        df.at[idx, \"ROUGE-L\"] = metrics_dict[\"ROUGE-L\"]\n",
    "        df.at[idx, \"Comprehensiveness\"] = compre_score\n",
    "        df.at[idx, \"Relevance\"] = rel_score\n",
    "        df.at[idx, \"Directness\"] = dir_score\n",
    "        df.at[idx, \"chrF\"] = metrics_dict[\"chrF\"]\n",
    "\n",
    "    return df  \n",
    "\n",
    "################\n",
    "# Example usage  \n",
    "################\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    df_in = pd.read_excel(r\"Outputs/LLM_responses_blazegraph_multiHop.xlsx\")\n",
    "\n",
    "    # Convert 'Retrieved result' to list of dicts\n",
    "    for idx, row in df_in.iterrows():\n",
    "        raw_ret = row[\"Blazegraph Response\"]\n",
    "        if isinstance(raw_ret, str):\n",
    "            try:\n",
    "                # parse JSON or literal\n",
    "                df_in.at[idx, \"Blazegraph Response\"] = json.loads(raw_ret)\n",
    "            except:\n",
    "                df_in.at[idx, \"Blazegraph Response\"] = []\n",
    "        elif pd.isna(raw_ret) or not isinstance(raw_ret, list):\n",
    "            df_in.at[idx, \"Blazegraph Response\"] = []\n",
    "    df_with_metrics = evaluate_dataset_per_row(df_in)\n",
    "    df_with_metrics.to_excel(r\"Outputs/LLM_metrics_blazegraph_multihop.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import six\n",
    "from types import SimpleNamespace\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "##############################################\n",
    "# 1) Azure OpenAI Client Initialization\n",
    "##############################################\\\n",
    "\n",
    "def load_config():\n",
    "    try:\n",
    "        with open(r\"config.json\") as f:\n",
    "            return json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Config file not found. Please check the path.\")\n",
    "\n",
    "def initialize_azure_client(config):\n",
    "    client = SecretClient(vault_url=config.key_vault_url, credential=DefaultAzureCredential())\n",
    "    secret = client.get_secret(config.dev_secret_name)\n",
    "    return AzureOpenAI(api_key=secret.value, api_version=config.chat.api_version, azure_endpoint=config.chat.azure_endpoint)\n",
    "\n",
    "config = load_config()\n",
    "llm = initialize_azure_client(config)\n",
    "\n",
    "\n",
    "##############################\n",
    "# 2) Normalization and Parsing helpers\n",
    "##############################\n",
    "\n",
    "def normalize_to_set(s):\n",
    "    \"\"\"\n",
    "    Tokenizes a string by:\n",
    "    1) Lowercasing\n",
    "    2) Removing punctuation\n",
    "    3) Removing URLs\n",
    "    4) Removing articles (a, an, the)\n",
    "    5) Collapsing extra whitespace\n",
    "    Returns a set of tokens.\n",
    "    \"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def remove_urls(text):\n",
    "        return re.sub(r'http[s]?://\\S+', '', text)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s) if s else \"\"\n",
    "    text = lower(s)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punc(text)\n",
    "    text = remove_articles(text)\n",
    "    text = white_space_fix(text)\n",
    "    tokens = text.split()\n",
    "    return set(tokens)\n",
    "\n",
    "def parse_documents_context(retrieved_text):\n",
    "    \"\"\"\n",
    "    Parses a string that represents a list of Document objects.\n",
    "    Extracts all `page_content` values via regex and joins them together.\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"page_content=['\\\"]([^'\\\"]+)['\\\"]\"\n",
    "    matches = re.findall(pattern, retrieved_text)\n",
    "    return \" \".join(matches)\n",
    "\n",
    "def parse_colon_delimited_data(doc_text):\n",
    "    \"\"\"\n",
    "    Parse colon-delimited blocks (e.g. \"PROCESS DATA:\", \"EMPLOYEE DATA:\", etc.)\n",
    "    and return a combined string of key/value tokens.\n",
    "    \"\"\"\n",
    "    blocks = doc_text.split('----')\n",
    "    all_tokens = []\n",
    "    possible_headers = [\"PROCESS DATA:\", \"EMPLOYEE DATA:\", \"APPLICATION DATA:\", \"ORGANISATION DATA:\"]\n",
    "    for block in blocks:\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "        for hdr in possible_headers:\n",
    "            if block.startswith(hdr):\n",
    "                block = block[len(hdr):].strip()\n",
    "                break\n",
    "        lines = block.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            segments = line.split('|')\n",
    "            for seg in segments:\n",
    "                seg = seg.strip()\n",
    "                if not seg:\n",
    "                    continue\n",
    "                kv = seg.split(':', 1)\n",
    "                if len(kv) == 2:\n",
    "                    key = kv[0].strip()\n",
    "                    val = kv[1].strip()\n",
    "                    all_tokens.append(key)\n",
    "                    all_tokens.append(val)\n",
    "                else:\n",
    "                    all_tokens.append(seg)\n",
    "    return \" \".join(all_tokens)\n",
    "\n",
    "def parse_triples(triples_text):\n",
    "    \"\"\"\n",
    "    Parse text assumed to be in triple format (subject predicate object per line)\n",
    "    and return the combined object tokens.\n",
    "    \"\"\"\n",
    "    lines = triples_text.split('\\n')\n",
    "    all_objects = []\n",
    "    for line in lines:\n",
    "        parts = line.split(maxsplit=2)\n",
    "        if len(parts) == 3:\n",
    "            obj_tokens = parts[2]\n",
    "            all_objects.append(obj_tokens)\n",
    "    return \" \".join(all_objects)\n",
    "\n",
    "def parse_subgraphs(subgraph_text):\n",
    "    \"\"\"\n",
    "    Parses subgraph text (each line representing a subgraph) and\n",
    "    returns the joined object tokens.\n",
    "    \"\"\"\n",
    "    subgraphs = re.split(r'(?i)Subgraph for .*', subgraph_text)\n",
    "    all_objects = []\n",
    "    for subg_block in subgraphs:\n",
    "        subg_block = subg_block.strip()\n",
    "        if not subg_block:\n",
    "            continue\n",
    "        lines = subg_block.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(maxsplit=2)\n",
    "            if len(parts) == 3:\n",
    "                obj_tokens = parts[2]\n",
    "                all_objects.append(obj_tokens)\n",
    "    return \" \".join(all_objects)\n",
    "\n",
    "def parse_retrieved_results(retrieved_text):\n",
    "    \"\"\"\n",
    "    Dynamically processes retrieved results:\n",
    "    1) If it contains \" DATA:\" then use parse_colon_delimited_data.\n",
    "    2) Else if it contains \"Subgraph for\" then use parse_subgraphs.\n",
    "    3) Else if it appears to be a list of Document objects (containing \"Document(\" and \"page_content\"),\n",
    "       then parse using parse_documents_context.\n",
    "    4) Otherwise parse as old-style triples.\n",
    "    \"\"\"\n",
    "    if not isinstance(retrieved_text, str):\n",
    "        retrieved_text = str(retrieved_text) if retrieved_text else \"\"\n",
    "\n",
    "    if \" DATA:\" in retrieved_text:\n",
    "        return parse_colon_delimited_data(retrieved_text)\n",
    "    elif \"Subgraph for\" in retrieved_text:\n",
    "        return parse_subgraphs(retrieved_text)\n",
    "    elif \"Document(\" in retrieved_text and \"page_content\" in retrieved_text:\n",
    "        return parse_documents_context(retrieved_text)\n",
    "    else:\n",
    "        return parse_triples(retrieved_text)\n",
    "\n",
    "##############################\n",
    "# 3) The LLM-based scoring functions\n",
    "##############################\n",
    "\n",
    "# Load metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def calculate_comprehensiveness(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of comprehensiveness.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    Comprehensiveness: \"How much detail does the answer provide to cover all the aspects and details of the\n",
    "    question? A comprehensive answer should be thorough and complete, without being redundant or irrelevant.\n",
    "    For example, if the question is 'What are the benefits and drawbacks of nuclear energy?', a comprehensive\n",
    "    answer would provide both the positive and negative aspects of nuclear energy, such as its efficiency,\n",
    "    environmental impact, safety, cost, etc. A comprehensive answer should not leave out any important points\n",
    "    or provide irrelevant information. For example, an incomplete answer would only provide the benefits of\n",
    "    nuclear energy without describing the drawbacks, or a redundant answer would repeat the same information\n",
    "    multiple times.\"\n",
    "\n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how comprehensive the response is. Where 0 is not comprehensive and 1 is the most comprehensive.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "\n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate relevance using LLM\n",
    "def calculate_relevance(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of relevance.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    Relevance: \"How well does the answer address the user query? A relevant answer should be directly related\n",
    "    to the question asked and provide information that is useful and on-topic. For example, if the question is\n",
    "    'What is the capital of France?', a relevant answer would be 'Paris'. A relevant answer should not provide\n",
    "    information that is unrelated to the question or that does not directly address the user's information need.\n",
    "    For example, an irrelevant answer would be 'The Eiffel Tower is located in Paris'.\"\n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how relevant the response is. Where 0 is not relevant and 1 is the most relevant.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "    \n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate directness using LLM\n",
    "def calculate_directness(query, llm_facts):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "    ---Role---\n",
    "    You are a helpful assistant responsible for evaluating the system response on the basis of directness.\n",
    "    ---Goal---\n",
    "    Given a user query and system response, assess if the response is suited according to\n",
    "    the following measure:\n",
    "    ---Measure---\n",
    "    directness: \"How specifically and clearly does the answer address the question? A direct answer should\n",
    "    provide a clear and concise answer to the question. For example, if the question is 'What is the capital\n",
    "    of France?', a direct answer would be 'Paris'. A direct answer should not provide any irrelevant or\n",
    "    unnecessary information that does not answer the question. For example, an indirect answer would be 'The\n",
    "    capital of France is located on the river Seine'.\",\n",
    "    \"empowerment\": \"How well does the answer help the reader understand and make informed judgements about\n",
    "    the topic without being misled or making fallacious assumptions. Evaluate each answer on the quality of\n",
    "    answer as it relates to clearly explaining and providing reasoning and sources behind the claims in the\n",
    "    answer.\"\n",
    "    Your assessment should include two parts:\n",
    "    Score: between 0 and 1 , depending how direct the response is. Where 0 is not direct and 1 is the most direct.\n",
    "    Reasoning: a short explanation of why you gave the score with respect to the measure described above.\n",
    "\n",
    "    ---query---\n",
    "    User query: {query}\n",
    "    ---system response---\n",
    "    System Response: {llm_facts}\n",
    "\n",
    "    \"\"\"\n",
    "        }]\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        score_line = next((line for line in result.splitlines() if line.lower().startswith(\"score:\")), None)\n",
    "        if score_line:\n",
    "            score = float(score_line.split(\":\")[1].strip())\n",
    "            return score\n",
    "        else:\n",
    "            raise ValueError(\"Score not found in response.\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        return 0\n",
    "\n",
    "###########################################\n",
    "# 4) chrF Helper Functions and Implementation\n",
    "###########################################\n",
    "\n",
    "def separate_characters(line):\n",
    "    return list(line.strip().replace(\" \", \"\"))\n",
    "\n",
    "def separate_punctuation(line):\n",
    "    words = line.strip().split()\n",
    "    tokenized = []\n",
    "    for w in words:\n",
    "        if len(w) == 1:\n",
    "            tokenized.append(w)\n",
    "        else:\n",
    "            lastChar = w[-1] \n",
    "            firstChar = w[0]\n",
    "            if lastChar in string.punctuation:\n",
    "                tokenized += [w[:-1], lastChar]\n",
    "            elif firstChar in string.punctuation:\n",
    "                tokenized += [firstChar, w[1:]]\n",
    "            else:\n",
    "                tokenized.append(w)\n",
    "    return tokenized\n",
    "\n",
    "def ngram_counts(wordList, order):\n",
    "    from collections import defaultdict\n",
    "    counts = defaultdict(lambda: defaultdict(float))\n",
    "    nWords = len(wordList)\n",
    "    for i in range(nWords):\n",
    "        for j in range(1, order+1):\n",
    "            if i+j <= nWords:\n",
    "                ngram = tuple(wordList[i:i+j])\n",
    "                counts[j-1][ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def ngram_matches(ref_ngrams, hyp_ngrams):\n",
    "    from collections import defaultdict\n",
    "    matchingNgramCount = defaultdict(float)\n",
    "    totalRefNgramCount = defaultdict(float)\n",
    "    totalHypNgramCount = defaultdict(float)\n",
    "    for order in ref_ngrams:\n",
    "        for ngram in hyp_ngrams[order]:\n",
    "            totalHypNgramCount[order] += hyp_ngrams[order][ngram]\n",
    "        for ngram in ref_ngrams[order]:\n",
    "            totalRefNgramCount[order] += ref_ngrams[order][ngram]\n",
    "            if ngram in hyp_ngrams[order]:\n",
    "                matchingNgramCount[order] += min(ref_ngrams[order][ngram], hyp_ngrams[order][ngram])\n",
    "    return matchingNgramCount, totalRefNgramCount, totalHypNgramCount\n",
    "\n",
    "def ngram_precrecf(matching, reflen, hyplen, beta):\n",
    "    factor = beta**2\n",
    "    totalPrec = sum(matching.values()) / sum(hyplen.values()) if sum(hyplen.values()) > 0 else 1e-16\n",
    "    totalRec = sum(matching.values()) / sum(reflen.values()) if sum(reflen.values()) > 0 else 1e-16\n",
    "    denom = factor * totalPrec + totalRec\n",
    "    totalF = ((1+factor)*totalPrec*totalRec / denom) if denom > 0 else 1e-16\n",
    "    return totalF\n",
    "\n",
    "def calculate_chrF(reference, hypothesis, nworder=2, ncorder=6, beta=2.0):\n",
    "    # Prepare tokens for word and character level evaluation\n",
    "    ref_words = separate_punctuation(reference)\n",
    "    hyp_words = separate_punctuation(hypothesis)\n",
    "    ref_chars = separate_characters(reference)\n",
    "    hyp_chars = separate_characters(hypothesis)\n",
    "\n",
    "    ref_word_ngram = ngram_counts(ref_words, nworder)\n",
    "    hyp_word_ngram = ngram_counts(hyp_words, nworder)\n",
    "    ref_char_ngram = ngram_counts(ref_chars, ncorder)\n",
    "    hyp_char_ngram = ngram_counts(hyp_chars, ncorder)\n",
    "\n",
    "    matching_word, ref_word_total, hyp_word_total = ngram_matches(ref_word_ngram, hyp_word_ngram)\n",
    "    matching_char, ref_char_total, hyp_char_total = ngram_matches(ref_char_ngram, hyp_char_ngram)\n",
    "\n",
    "    word_fscore = ngram_precrecf(matching_word, ref_word_total, hyp_word_total, beta)\n",
    "    char_fscore = ngram_precrecf(matching_char, ref_char_total, hyp_char_total, beta)\n",
    "\n",
    "    total_fscore = (word_fscore + char_fscore) / 2\n",
    "    return total_fscore\n",
    "\n",
    "\n",
    "def evaluate_single(gold_str, retrieved_str, final_answer_str):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics:\n",
    "    - Accuracy\n",
    "    - Precision/Recall (retrieved tokens)\n",
    "    - F1 Score\n",
    "    - Hits@1\n",
    "    - BLEU and ROUGE scores\n",
    "    - chrF score\n",
    "    \"\"\"\n",
    "    if not isinstance(retrieved_str, str):\n",
    "        retrieved_str = str(retrieved_str) if retrieved_str else \"\"\n",
    "    if not isinstance(final_answer_str, str):\n",
    "        final_answer_str = str(final_answer_str) if final_answer_str else \"\"\n",
    "    \n",
    "    gold_tokens = normalize_to_set(gold_str)\n",
    "    parsed_objects_str = parse_retrieved_results(retrieved_str)\n",
    "    retrieved_tokens = normalize_to_set(parsed_objects_str)\n",
    "    final_ans_tokens = normalize_to_set(final_answer_str)\n",
    "\n",
    "    print(\"Gold tokens:\", gold_tokens)\n",
    "    print(\"Retrieved tokens:\", retrieved_tokens)\n",
    "    print(\"Final answer tokens:\", final_ans_tokens)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 1.0 if gold_tokens.issubset(final_ans_tokens) else 0.0\n",
    "\n",
    "    # Precision / Recall\n",
    "    intersection_retrieved = retrieved_tokens.intersection(gold_tokens)\n",
    "    num_common_ret = len(intersection_retrieved)\n",
    "    p_ret = num_common_ret / len(retrieved_tokens) if retrieved_tokens else 1.0\n",
    "    r_ret = num_common_ret / len(gold_tokens) if gold_tokens else 1.0\n",
    "    f1_score = 2 * (p_ret * r_ret) / (p_ret + r_ret) if (p_ret + r_ret) > 0 else 0.0\n",
    "\n",
    "    # Hits@1: if any common token is found, count as hit\n",
    "    hits1 = 1.0 if num_common_ret > 0 else 0.0\n",
    "\n",
    "    # BLEU and ROUGE calculations (using normalized and sorted tokens)\n",
    "    predicted_sentence = \" \".join(sorted(final_ans_tokens))\n",
    "    ground_truth_sentence = \" \".join(sorted(gold_tokens))\n",
    "    try:\n",
    "        bleu_result = bleu_metric.compute(predictions=[predicted_sentence], references=[[ground_truth_sentence]])\n",
    "        rouge_result = rouge_metric.compute(predictions=[predicted_sentence], references=[ground_truth_sentence])\n",
    "        bleu_score = bleu_result[\"bleu\"]\n",
    "        rouge_1 = rouge_result[\"rouge1\"]\n",
    "        rouge_2 = rouge_result[\"rouge2\"]\n",
    "        rouge_l = rouge_result[\"rougeL\"]\n",
    "        chrf_score = calculate_chrF(ground_truth_sentence, predicted_sentence)\n",
    "    except ZeroDivisionError:\n",
    "        bleu_score = rouge_1 = rouge_2 = rouge_l = chrf_score = 0\n",
    "\n",
    "    print(\"Intersection (retrieved vs gold):\", intersection_retrieved)\n",
    "    print(\"Precision (retrieved):\", p_ret)\n",
    "    print(\"Recall (retrieved):\", r_ret)\n",
    "    print(\"F1 Score (retrieved):\", f1_score)\n",
    "    print(\"Hits@1:\", hits1)\n",
    "    print(\"BLEU, ROUGE1, ROUGE2, ROUGEL, chrf_score:\", bleu_score, rouge_1, rouge_2, rouge_l, chrf_score)\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision(Retrieved)': p_ret,\n",
    "        'Recall(Retrieved)': r_ret,\n",
    "        'F1': f1_score,\n",
    "        'Hits': hits1,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_1,\n",
    "        \"ROUGE-2\": rouge_2,\n",
    "        \"ROUGE-L\": rouge_l,\n",
    "        \"chrF Score\": chrf_score\n",
    "    }\n",
    "\n",
    "##############################\n",
    "# 5) Example evaluation over an Excel file\n",
    "##############################\n",
    "\n",
    "def evaluate_topk_excel(input_excel, output_excel):\n",
    "    \"\"\"\n",
    "    Reads columns including:\n",
    "      - 'Query'\n",
    "      - 'Ground Truth Answer'\n",
    "      - 'Top-K Subgraphs' or 'Top-K Answer' or 'Top-K Triples' etc.\n",
    "\n",
    "    For each row, computes evaluation metrics and writes them to new columns.\n",
    "    \"\"\"\n",
    "    top_k_values = [1,3,5,8,13,15,21]\n",
    "    df = pd.read_excel(input_excel)\n",
    "    col_names = [\n",
    "        'Accuracy',\n",
    "        'Precision(Retrieved)',\n",
    "        'Recall(Retrieved)',\n",
    "        'Hits',\n",
    "        \"BLEU Score\",\n",
    "        \"ROUGE-1\",\n",
    "        \"ROUGE-2\",\n",
    "        \"ROUGE-L\",\n",
    "        \"chrF Score\"\n",
    "    ]\n",
    "    for k in top_k_values:\n",
    "        for c in col_names:\n",
    "            df[f\"{c}@{k}\"] = None\n",
    "    df[\"MRR\"] = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        gold_str = row.get(\"Ground Truth Answer\", \"\")\n",
    "        gold_tokens = normalize_to_set(gold_str)\n",
    "        rank = None\n",
    "        for k in top_k_values:\n",
    "            triple_key = f\"Top-{k} Subgraphs\"\n",
    "            answer_key = f\"Top-{k} Answer\"\n",
    "            if triple_key not in df.columns or answer_key not in df.columns:\n",
    "                continue\n",
    "            retrieved_str = row.get(triple_key, \"\")\n",
    "            final_ans_str = row.get(answer_key, \"\")\n",
    "            parsed_objects_str = parse_retrieved_results(retrieved_str)\n",
    "            retrieved_tokens = normalize_to_set(parsed_objects_str)\n",
    "            metrics = evaluate_single(gold_str, retrieved_str, final_ans_str)\n",
    "            for c in col_names:\n",
    "                df.at[i, f\"{c}@{k}\"] = metrics[c]\n",
    "            if k == 13:\n",
    "                query = row.get(\"Query\", \"\")\n",
    "                df.at[i, f\"Comprehensiveness@{k}\"] = calculate_comprehensiveness(query, final_ans_str)\n",
    "                df.at[i, f\"Relevance@{k}\"] = calculate_relevance(query, final_ans_str)\n",
    "                df.at[i, f\"Directness@{k}\"] = calculate_directness(query, final_ans_str)\n",
    "            if rank is None and gold_tokens.issubset(retrieved_tokens):\n",
    "                rank = k\n",
    "        df.at[i, \"MRR\"] = 1.0 / rank if rank else 0.0\n",
    "\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Done. Metrics written to {output_excel}\")\n",
    "\n",
    "#########################################\n",
    "# 5) Example usage with an Excel file as input\n",
    "#########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data for testing\n",
    "    # data = [\n",
    "    #     {\n",
    "    #         \"Query\": \"What is Michael Woerther's GID\",\n",
    "    #         \"Ground Truth Answer\": \"Z002H0WF\",\n",
    "    #         \"Final Answer\": \"Michael Wörther's GID is **Z002H0WF**.\",\n",
    "    #         \"Retrieved response\": \"\"\"[Document(metadata={'source': ['https://example.com/1'], 'search_space': 'organization'}, page_content='Z002H0WF is the gid of Michael Wörther'), Document(metadata={'source': ['https://example.com/2'], 'search_space': 'organization'}, page_content='Z000MJDW is the gid of Michael Goerz'), Document(metadata={'source': ['https://example.com/3'], 'search_space': 'organization'}, page_content='Z0006BDZ is the gid of Michael Wedemeyer'), Document(metadata={'source': ['https://example.com/4'], 'search_space': 'organization'}, page_content='Z000KR4F is the gid of Michael Hölzl'), Document(metadata={'source': ['https://example.com/5'], 'search_space': 'organization'}, page_content='Z000GGGG is the gid of Michaela Lehning')]\"\"\"\n",
    "    #     }\n",
    "    # ]\n",
    "    # df = pd.DataFrame(data)\n",
    "    # # Evaluate a single row (for demonstration)\n",
    "    # for i, row in df.iterrows():\n",
    "    #     metrics_dict = evaluate_single(row[\"Ground Truth Answer\"], row[\"Retrieved response\"], row[\"Final Answer\"])\n",
    "    #     print(\"Evaluation Metrics:\", metrics_dict)\n",
    "\n",
    "    # Evaluate an entire dataset\n",
    "    input_file = r\"Outputs/LLM_metrics_subgraph_multiHop.xlsx\"\n",
    "    output_file = r\"Outputs/LLM_metrics_subgraph_multiHop_complete.xlsx\"\n",
    "    evaluate_topk_excel(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
