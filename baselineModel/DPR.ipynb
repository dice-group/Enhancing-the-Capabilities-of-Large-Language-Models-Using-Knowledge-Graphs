{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import json \n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import faiss \n",
    "from types import SimpleNamespace \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" \n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI \n",
    "\n",
    "\n",
    "# --------------------------------------------------- \n",
    "# 1. Load the DPR Models and Tokenizers \n",
    "# --------------------------------------------------- \n",
    "from transformers import ( \n",
    "    DPRContextEncoder, \n",
    "    DPRContextEncoderTokenizer, \n",
    "    DPRQuestionEncoder, \n",
    "    DPRQuestionEncoderTokenizer \n",
    ") \n",
    "\n",
    "print(\"Loading DPR models...\") \n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\") \n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\") \n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\") \n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 2. Load CSV Files (data sources) \n",
    "# --------------------------------------------------- \n",
    "app_file = r\"data/application_master.csv\"   # Application data\n",
    "app_emp_file = r\"data/apps_owners.csv\"       # Application connecting with employees\n",
    "emp_file = r\"data/Fin_Emp.csv\"               # Employee data\n",
    "org_file = r\"data/Fin_Org.csv\"               # Organisation connecting with employees\n",
    "proc_file = r\"data/process_master.csv\"     # Process data\n",
    "proc_org_file = r\"data/orgs_processes.csv\" # Process connecting with organisation\n",
    "proc_app_file = r\"data/process_applications.csv\"  # Processes connecting applications\n",
    "proc_emp_file = r\"data/process_owners.csv\"       # Process connecting with employees\n",
    "\n",
    "print(\"Loading CSV files...\") \n",
    "app_df = pd.read_csv(app_file) \n",
    "app_emp_df = pd.read_csv(app_emp_file) \n",
    "emp_df = pd.read_csv(emp_file) \n",
    "org_df = pd.read_csv(org_file) \n",
    "proc_df = pd.read_csv(proc_file) \n",
    "proc_org_df = pd.read_csv(proc_org_file) \n",
    "proc_app_df = pd.read_csv(proc_app_file) \n",
    "proc_emp_df = pd.read_csv(proc_emp_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 3. Helper Function to Create a Document \n",
    "# --------------------------------------------------- \n",
    "def create_document(entity_type, row, extra_info=\"\"): \n",
    "    \"\"\" \n",
    "    Flatten a row into a document string. \n",
    "    entity_type: e.g., \"application\", \"employee\", \"organisation\", \"process\" \n",
    "    row: a pandas Series containing the entity data. \n",
    "    extra_info: additional linking information. \n",
    "    Returns a dict with doc_id, text, and metadata. \n",
    "    \"\"\" \n",
    "    fields = [f\"{col}: {row[col]}\" for col in row.index if pd.notnull(row[col])] \n",
    "    doc_text = f\"{entity_type.upper()} DATA: \" + \" | \".join(fields) \n",
    "    if extra_info: \n",
    "        doc_text += \" | \" + extra_info \n",
    "    metadata = {\"entity_type\": entity_type, \"id\": row.get(\"id\", None)} \n",
    "    return {\"doc_id\": f\"{entity_type}_{row.get('id', '')}\", \"text\": doc_text, \"metadata\": metadata} \n",
    "\n",
    "documents = [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 195537 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 4. Create Documents for Each Entity (with linking info) \n",
    "# --------------------------------------------------- \n",
    "# --- Application Documents --- \n",
    "for _, row in app_df.iterrows(): \n",
    "    extra_parts = [] \n",
    "    linked_emp = app_emp_df[app_emp_df['app_id'] == row['id']] \n",
    "    if not linked_emp.empty: \n",
    "        emp_info = [] \n",
    "        for _, lrow in linked_emp.iterrows(): \n",
    "            emp_info.append(f\"employee_id: {lrow['employee_id']} (is_owner: {lrow['is_owners']})\") \n",
    "        extra_parts.append(\"Linked employees: \" + \", \".join(emp_info)) \n",
    "    extra_parts.append(\"App Org: \" + str(row.get(\"app_org\", \"\"))) \n",
    "    extra = \" | \".join(extra_parts) \n",
    "    documents.append(create_document(\"application\", row, extra)) \n",
    "\n",
    "\n",
    "# --- Employee Documents --- \n",
    "for _, row in emp_df.iterrows(): \n",
    "    extra = f\"Org ID: {row.get('org_id', '')}, Line Manager ID: {row.get('line_manager_id', '')}\" \n",
    "    documents.append(create_document(\"employee\", row, extra)) \n",
    "\n",
    "\n",
    "# --- Organisation Documents --- \n",
    "for _, row in org_df.iterrows(): \n",
    "    extra = f\"Org Head: {row.get('org_head', '')}, Parent Org ID: {row.get('parent_org_id', '')}\" \n",
    "    documents.append(create_document(\"organisation\", row, extra)) \n",
    "\n",
    "\n",
    "# --- Process Documents --- \n",
    "for _, row in proc_df.iterrows(): \n",
    "    extra_parts = [] \n",
    "    linked_org = proc_org_df[proc_org_df['process_id'] == row['id']] \n",
    "    if not linked_org.empty: \n",
    "        org_ids = [f\"org_id: {r['org_id']}\" for _, r in linked_org.iterrows()] \n",
    "        extra_parts.append(\"Linked Organisation(s): \" + \", \".join(org_ids)) \n",
    "    linked_app = proc_app_df[proc_app_df['process_id'] == row['id']] \n",
    "    if not linked_app.empty: \n",
    "        app_ids = [f\"application_id: {r['application_id']}\" for _, r in linked_app.iterrows()] \n",
    "        extra_parts.append(\"Linked Application(s): \" + \", \".join(app_ids)) \n",
    "    linked_emp = proc_emp_df[proc_emp_df['process_id'] == row['id']] \n",
    "    if not linked_emp.empty: \n",
    "        emp_ids = [f\"employee_id: {r['employee_id']} (is_owner: {r['is_owners']})\" for _, r in linked_emp.iterrows()] \n",
    "        extra_parts.append(\"Linked Employee(s): \" + \", \".join(emp_ids)) \n",
    "    extra = \" | \".join(extra_parts) \n",
    "    documents.append(create_document(\"process\", row, extra)) \n",
    "\n",
    "print(f\"Created {len(documents)} documents.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 5. Encode Documents Using the DPR Context Encoder \n",
    "# --------------------------------------------------- \n",
    "def encode_documents(doc_texts, batch_size=16): \n",
    "    embeddings = [] \n",
    "    for i in range(0, len(doc_texts), batch_size): \n",
    "        batch_texts = doc_texts[i: i + batch_size] \n",
    "        inputs = ctx_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512) \n",
    "        with torch.no_grad(): \n",
    "            model_output = ctx_encoder(**inputs) \n",
    "        batch_embeddings = model_output.pooler_output.cpu().numpy() \n",
    "        embeddings.append(batch_embeddings) \n",
    "    embeddings = np.vstack(embeddings) \n",
    "    return embeddings \n",
    "\n",
    "\n",
    "print(\"Encoding documents...\") \n",
    "doc_texts = [doc[\"text\"] for doc in documents] \n",
    "doc_embeddings = encode_documents(doc_texts) \n",
    "\n",
    "\n",
    "# Normalize embeddings for inner product search \n",
    "faiss.normalize_L2(doc_embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Faiss index...\n",
      "Indexed 195537 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 6. Build the Faiss Index \n",
    "# --------------------------------------------------- \n",
    "d = doc_embeddings.shape[1] \n",
    "print(\"Building Faiss index...\") \n",
    "index = faiss.IndexFlatIP(d)  # inner product search on normalized embeddings \n",
    "index.add(doc_embeddings) \n",
    "print(f\"Indexed {index.ntotal} documents.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 7. Define Query Retrieval Function \n",
    "# --------------------------------------------------- \n",
    "def retrieve(query, top_k): \n",
    "    inputs = question_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=128) \n",
    "    with torch.no_grad(): \n",
    "        q_output = question_encoder(**inputs) \n",
    "    q_embedding = q_output.pooler_output.cpu().numpy() \n",
    "    faiss.normalize_L2(q_embedding) \n",
    "    distances, indices = index.search(q_embedding, top_k) \n",
    "    results = [] \n",
    "    for idx in indices[0]: \n",
    "        results.append(documents[idx]) \n",
    "    return results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 8. LLM Functions (Azure OpenAI) \n",
    "# --------------------------------------------------- \n",
    "def load_config(): \n",
    "\n",
    "    config_path = r\"config.json\" \n",
    "    try: \n",
    "        with open(config_path) as f: \n",
    "            config = json.load(f, object_hook=lambda d: SimpleNamespace(**d)) \n",
    "        return config \n",
    "    except FileNotFoundError: \n",
    "        raise FileNotFoundError(\"Config file not found. Please check the path.\") \n",
    "\n",
    "def initialize_azure_client(config): \n",
    "    \"\"\"Initialize Azure KeyVault and Azure OpenAI client.\"\"\" \n",
    "    client = SecretClient(vault_url=config.key_vault_url, credential=DefaultAzureCredential()) \n",
    "    secret = client.get_secret(config.dev_secret_name) \n",
    "    return AzureOpenAI(api_key=secret.value, \n",
    "                    api_version=config.chat.api_version, \n",
    "                    azure_endpoint=config.chat.azure_endpoint) \n",
    "\n",
    "\n",
    "def generate_answer_with_llm(query: str, top_documents): \n",
    "    \"\"\"\n",
    "    Use Azure OpenAI to generate a final answer from the top retrieved documents. \n",
    "    \"\"\" \n",
    "    config = load_config() \n",
    "    llm = initialize_azure_client(config) \n",
    "    context = \"\\n\\n\".join(top_documents) \n",
    "    prompt = [ \n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"\"\" \n",
    "You are an AI assistant tasked with answering a query based on the provided context about employees and organizations. \n",
    "Please provide a detailed and well-structured answer to the user's question. \n",
    "\n",
    "- Organize the answer into bullet points if appropriate. \n",
    "- Use headings where relevant. \n",
    "- Include all relevant details concisely. \n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Question: \"{query}\" \n",
    "\n",
    "Provide a well-structured answer. \n",
    "            \"\"\" \n",
    "        } \n",
    "    ] \n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt) \n",
    "    response_content = response.choices[0].message.content.strip() \n",
    "    return response_content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 9. Process Queries from an Excel File and Save Results \n",
    "# --------------------------------------------------- \n",
    "query_excel_file = r\"data/LLMEval_1.xlsx\"       \n",
    "output_excel_file = r\"Outputs/LLM_responses_dpr_multihop.xlsx\" \n",
    "\n",
    "queries_df = pd.read_excel(query_excel_file) \n",
    "# List of k values to test. \n",
    "k_values = [1, 2, 3, 5, 8, 13, 15, 21] \n",
    "\n",
    "# For each query, for each k, retrieve docs, record response time, generate final answer. \n",
    "for idx, row in queries_df.iterrows(): \n",
    "    query = row[\"Query\"] \n",
    "    for k in k_values: \n",
    "        col_docs = f\"k_{k}_docs\" \n",
    "        col_time = f\"k_{k}_retrieve_time\" \n",
    "        col_responsetime = f\"k_{k}_response_answer\"\n",
    "        col_answer = f\"k_{k}_final_answer\" \n",
    "        \n",
    "        start_time = time.time() \n",
    "        retrieved_docs = retrieve(query, k) \n",
    "        elapsed_time = time.time() - start_time \n",
    "        \n",
    "        # Prepare a string with the document IDs. \n",
    "        docs_str = \"\\n\\n----\\n\\n \".join([doc[\"text\"] for doc in retrieved_docs]) \n",
    "        \n",
    "        # Generate final answer using Azure OpenAI. \n",
    "        # We use the top k documents retrieved.\n",
    "        top_docs_text = [doc[\"text\"] for doc in retrieved_docs] \n",
    "        start_time = time.time() \n",
    "        final_answer = generate_answer_with_llm(query, top_docs_text) \n",
    "        response_time = time.time() - start_time\n",
    "        # Save the results in the DataFrame. \n",
    "        queries_df.at[idx, col_docs] = docs_str \n",
    "        queries_df.at[idx, col_time] = elapsed_time \n",
    "        queries_df.at[idx, col_responsetime] = response_time\n",
    "        queries_df.at[idx, col_answer] = final_answer \n",
    "        print(f\"Processed query '{query[:50]}...' for k={k} in {elapsed_time:.2f} seconds.\") \n",
    "\n",
    "\n",
    "# Save the updated DataFrame with new columns to a new Excel file. \n",
    "queries_df.to_excel(output_excel_file, index=False) \n",
    "print(f\"Results saved to {output_excel_file}\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
