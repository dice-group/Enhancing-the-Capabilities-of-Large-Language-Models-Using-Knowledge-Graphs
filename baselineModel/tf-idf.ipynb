{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "TF‑IDF Retrieval-Augmented Generation Pipeline \n",
    "This script:\n",
    "  1. Loads CSV files (application, employee, organisation, process, etc.)\n",
    "  2. Creates document strings for each entity.\n",
    "  3. Builds a TF‑IDF representation of all documents.\n",
    "  4. Reads queries from an Excel file.\n",
    "  5. For each query and for each specified top-k value:\n",
    "       • Retrieves the top‑k documents (using TF‑IDF cosine similarity)\n",
    "       • Generates a final answer using an LLM \n",
    "       • Records retrieval and LLM generation (response) times.\n",
    "  6. Writes the results (retrieved document texts, times, final answer) into an output Excel file.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from types import SimpleNamespace \n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI \n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load CSV Files and Create Documents\n",
    "# -------------------------------\n",
    "app_file = r\"data/application_master.csv\"   # Application data\n",
    "app_emp_file = r\"data/apps_owners.csv\"       # Application connecting with employees\n",
    "emp_file = r\"data/Fin_Emp.csv\"               # Employee data\n",
    "org_file = r\"data/Fin_Org.csv\"               # Organisation connecting with employees\n",
    "proc_file = r\"data/process_master.csv\"     # Process data\n",
    "proc_org_file = r\"data/orgs_processes.csv\" # Process connecting with organisation\n",
    "proc_app_file = r\"data/process_applications.csv\"  # Processes connecting applications\n",
    "proc_emp_file = r\"data/process_owners.csv\"       # Process connecting with employees\n",
    "\n",
    "print(\"Loading CSV files...\")\n",
    "app_df      = pd.read_csv(app_file)\n",
    "app_emp_df  = pd.read_csv(app_emp_file)\n",
    "emp_df      = pd.read_csv(emp_file)\n",
    "org_df      = pd.read_csv(org_file)\n",
    "proc_df     = pd.read_csv(proc_file)\n",
    "proc_org_df = pd.read_csv(proc_org_file)\n",
    "proc_app_df = pd.read_csv(proc_app_file)\n",
    "proc_emp_df = pd.read_csv(proc_emp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_document(entity_type, row, extra_info=\"\"):\n",
    "    \"\"\"Flatten a pandas Series into a document string.\"\"\"\n",
    "    fields = [f\"{col}: {row[col]}\" for col in row.index if pd.notnull(row[col])]\n",
    "    doc_text = f\"{entity_type.upper()} DATA: \" + \" | \".join(fields)\n",
    "    if extra_info:\n",
    "        doc_text += \" | \" + extra_info\n",
    "    metadata = {\"entity_type\": entity_type, \"id\": row.get(\"id\", None)}\n",
    "    return {\"doc_id\": f\"{entity_type}_{row.get('id', '')}\", \"text\": doc_text, \"metadata\": metadata}\n",
    "\n",
    "documents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 195537 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Application Documents ---\n",
    "for _, row in app_df.iterrows():\n",
    "    extra_parts = []\n",
    "    linked_emp = app_emp_df[app_emp_df['app_id'] == row['id']]\n",
    "    if not linked_emp.empty:\n",
    "        emp_info = []\n",
    "        for _, lrow in linked_emp.iterrows():\n",
    "            emp_info.append(f\"employee_id: {lrow['employee_id']} (is_owner: {lrow['is_owners']})\")\n",
    "        extra_parts.append(\"Linked employees: \" + \", \".join(emp_info))\n",
    "    extra_parts.append(\"App Org: \" + str(row.get(\"app_org\", \"\")))\n",
    "    extra = \" | \".join(extra_parts)\n",
    "    documents.append(create_document(\"application\", row, extra))\n",
    "\n",
    "# --- Employee Documents ---\n",
    "for _, row in emp_df.iterrows():\n",
    "    extra = f\"Org ID: {row.get('org_id', '')}, Line Manager ID: {row.get('line_manager_id', '')}\"\n",
    "    documents.append(create_document(\"employee\", row, extra))\n",
    "\n",
    "# --- Organisation Documents ---\n",
    "for _, row in org_df.iterrows():\n",
    "    extra = f\"Org Head: {row.get('org_head', '')}, Parent Org ID: {row.get('parent_org_id', '')}\"\n",
    "    documents.append(create_document(\"organisation\", row, extra))\n",
    "\n",
    "# --- Process Documents ---\n",
    "for _, row in proc_df.iterrows():\n",
    "    extra_parts = []\n",
    "    linked_org = proc_org_df[proc_org_df['process_id'] == row['id']]\n",
    "    if not linked_org.empty:\n",
    "        org_ids = [f\"org_id: {r['org_id']}\" for _, r in linked_org.iterrows()]\n",
    "        extra_parts.append(\"Linked Organisation(s): \" + \", \".join(org_ids))\n",
    "    linked_app = proc_app_df[proc_app_df['process_id'] == row['id']]\n",
    "    if not linked_app.empty:\n",
    "        app_ids = [f\"application_id: {r['application_id']}\" for _, r in linked_app.iterrows()]\n",
    "        extra_parts.append(\"Linked Application(s): \" + \", \".join(app_ids))\n",
    "    linked_emp = proc_emp_df[proc_emp_df['process_id'] == row['id']]\n",
    "    if not linked_emp.empty:\n",
    "        emp_ids = [f\"employee_id: {r['employee_id']} (is_owner: {r['is_owners']})\" for _, r in linked_emp.iterrows()]\n",
    "        extra_parts.append(\"Linked Employee(s): \" + \", \".join(emp_ids))\n",
    "    extra = \" | \".join(extra_parts)\n",
    "    documents.append(create_document(\"process\", row, extra))\n",
    "\n",
    "print(f\"Created {len(documents)} documents.\")\n",
    "\n",
    "# Create a list of document texts\n",
    "doc_texts = [doc[\"text\"] for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 2. Build TF‑IDF Retrieval Pipeline\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Building TF‑IDF index...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, stop_words=\"english\")\n",
    "doc_tfidf = tfidf_vectorizer.fit_transform(doc_texts) \n",
    "print(\"TF‑IDF index built.\")\n",
    "\n",
    "def retrieve_tfidf(query, top_k=5):\n",
    "    \"\"\"Retrieve top-k documents for a query using cosine similarity on TF‑IDF vectors.\"\"\"\n",
    "    query_vec = tfidf_vectorizer.transform([query])\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = (doc_tfidf * query_vec.T).toarray().flatten()\n",
    "    top_indices = np.argsort(cosine_sim)[::-1][:top_k]\n",
    "    results = [documents[i] for i in top_indices]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------- \n",
    "# 3. LLM Functions (Azure OpenAI) \n",
    "# --------------------------------------------------- \n",
    "def load_config(): \n",
    "    config_path = r\"config.json\" \n",
    "    try: \n",
    "        with open(config_path) as f: \n",
    "            config = json.load(f, object_hook=lambda d: SimpleNamespace(**d)) \n",
    "        return config \n",
    "    except FileNotFoundError: \n",
    "        raise FileNotFoundError(\"Config file not found. Please check the path.\") \n",
    "\n",
    "def initialize_azure_client(config): \n",
    "    \"\"\"Initialize Azure KeyVault and Azure OpenAI client.\"\"\" \n",
    "    client = SecretClient(vault_url=config.key_vault_url, credential=DefaultAzureCredential()) \n",
    "    secret = client.get_secret(config.dev_secret_name) \n",
    "    return AzureOpenAI(api_key=secret.value, \n",
    "                    api_version=config.chat.api_version, \n",
    "                    azure_endpoint=config.chat.azure_endpoint) \n",
    "\n",
    "\n",
    "def generate_answer_with_llm(query: str, top_documents): \n",
    "    \"\"\"\n",
    "    Use Azure OpenAI to generate a final answer from the top retrieved documents. \n",
    "    \"\"\" \n",
    "    config = load_config() \n",
    "    llm = initialize_azure_client(config) \n",
    "    context = \"\\n\\n\".join(top_documents) \n",
    "    prompt = [ \n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"\"\" \n",
    "You are an AI assistant tasked with answering a query based on the provided context about employees and organizations. \n",
    "Please provide a detailed and well-structured answer to the user's question. \n",
    "\n",
    "- Organize the answer into bullet points if appropriate. \n",
    "- Use headings where relevant. \n",
    "- Include all relevant details concisely. \n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Question: \"{query}\" \n",
    "\n",
    "Provide a well-structured answer. \n",
    "            \"\"\" \n",
    "        } \n",
    "    ] \n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt) \n",
    "    response_content = response.choices[0].message.content.strip() \n",
    "    return response_content \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test query\n",
    "example_query = \"What is the email address of Anubhuti Singh?\"\n",
    "\n",
    "# Retrieve top-k documents for the example query\n",
    "retrieved_docs = retrieve_tfidf(example_query, top_k=5)\n",
    "\n",
    "print(\"Retrieved Documents :\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"Doc ID: {doc['doc_id']}\")\n",
    "    print(doc[\"text\"][:150] + \"...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Generate an answer using the LLM simulation function\n",
    "top_docs_texts = [doc[\"text\"] for doc in retrieved_docs]\n",
    "generated_answer = generate_answer_with_llm(example_query, top_docs_texts)\n",
    "\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 4. Process Queries from Excel File\n",
    "# -------------------------------\n",
    "\n",
    "query_excel_file = r\"data/LLMEval_1.xlsx\"      \n",
    "output_excel_file = r\"Outputs/LLM_responses_tf-idf_multihop.xlsx\"\n",
    "\n",
    "print(\"Loading query Excel file...\")\n",
    "queries_df = pd.read_excel(query_excel_file)\n",
    "\n",
    "# List of k values to test.\n",
    "k_values = [1, 3, 5, 8, 13, 15, 21]\n",
    "\n",
    "# For each query, for each k, retrieve docs, generate final answer, and record timings.\n",
    "for idx, row in queries_df.iterrows():\n",
    "    query = row[\"Query\"]\n",
    "    for k in k_values:\n",
    "        col_docs = f\"k_{k}_docs\"\n",
    "        col_retrieval_time = f\"k_{k}_retrieve_time\"\n",
    "        col_response_time = f\"k_{k}_response_time\"\n",
    "        col_final_answer = f\"k_{k}_final_answer\"\n",
    "        # Retrieve documents using TF‑IDF.\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = retrieve_tfidf(query, top_k=k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        # Prepare a string with the top retrieved document texts.\n",
    "        docs_str = \"\\n\\n----\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "        top_docs_texts = [doc[\"text\"] for doc in retrieved_docs]\n",
    "        # Generate answer using the LLM.\n",
    "        start_time = time.time()\n",
    "        final_answer = generate_answer_with_llm(query, top_docs_texts)\n",
    "        print(final_answer)\n",
    "        response_time = time.time() - start_time\n",
    "        # Save results in the DataFrame.\n",
    "        queries_df.at[idx, col_docs] = docs_str\n",
    "        queries_df.at[idx, col_retrieval_time] = retrieval_time\n",
    "        queries_df.at[idx, col_response_time] = response_time\n",
    "        queries_df.at[idx, col_final_answer] = final_answer\n",
    "        print(f\"Processed query '{query[:50]}...' for k={k}: retrieval {retrieval_time:.2f}s, response {response_time:.2f}s.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save Results to Excel\n",
    "# -------------------------------\n",
    "queries_df.to_excel(output_excel_file, index=False)\n",
    "print(f\"Results saved to {output_excel_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
