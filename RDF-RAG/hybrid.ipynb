{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence_transformers \n",
    "# rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted C:\\Users\\z0050t3j\\OneDrive - Siemens Energy\\Dokumente\\Thesis\\anubhuti_master_thesis\\kgCreation\\ExtendedFinKG_Pro.ttl to C:\\Users\\z0050t3j\\OneDrive - Siemens Energy\\Dokumente\\Thesis\\anubhuti_master_thesis\\kgCreation\\FinKGTripleLatestPro.txt.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "input_file = r\"kgCreation/ExtendedFinKG_Pro.ttl\"\n",
    "output_file = r\"kgCreation/FinKGTripleLatestPro.txt\"\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse(input_file, format = 'turtle')\n",
    "\n",
    "with open(output_file, 'w', encoding= \"utf-8\") as file:\n",
    "    for subject, predicate, obj in graph:\n",
    "        triple_line = f\"{subject} {predicate} {obj}\"\n",
    "        file.write(triple_line + \"\\n\")\n",
    "        \n",
    "print(f\"Successfully converted {input_file} to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity subgraphs saved to C:\\Users\\z0050t3j\\OneDrive - Siemens Energy\\Dokumente\\Thesis\\anubhuti_master_thesis\\kgCreation\\FinSubGraphLatestPro.txt\n"
     ]
    }
   ],
   "source": [
    "# Load triples from the text file and organize them into subgraphs\n",
    "file_path = r\"kgCreation/FinKGTripleLatestPro.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf=8\") as file:\n",
    "    triples = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "subgraphs = {}\n",
    "for triple in triples:\n",
    "    parts = triple.split()\n",
    "    subject = parts[0]\n",
    "    if subject not in subgraphs:\n",
    "        subgraphs[subject] = []\n",
    "    subgraphs[subject].append(triple)\n",
    "\n",
    "# Save each entity's subgraph as a block in a single text file\n",
    "subgraph_file_path = r\"kgCreation/FinSubGraphLatestPro.txt\"\n",
    "with open(subgraph_file_path, \"w\", encoding=\"utf=8\") as subgraph_file:\n",
    "    for entity, triples in subgraphs.items():\n",
    "        subgraph_file.write(f\"Subgraph for {entity}:\\n\")\n",
    "        subgraph_file.write(\"\\n\".join(triples) + \"\\n\\n\")\n",
    "print(f\"Entity subgraphs saved to {subgraph_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\z0050t3j\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from types import SimpleNamespace\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "\n",
    "# ------------------------- Configuration and Initialization -------------------------\n",
    "def load_config():\n",
    "    try:\n",
    "        with open(r\"config.json\") as f:\n",
    "            return json.load(f, object_hook=lambda d: SimpleNamespace(**d))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Config file not found. Please check the path.\")\n",
    "\n",
    "def initialize_azure_client(config):\n",
    "    client = SecretClient(vault_url=config.key_vault_url, credential=DefaultAzureCredential())\n",
    "    secret = client.get_secret(config.dev_secret_name)\n",
    "    return AzureOpenAI(api_key=secret.value, api_version=config.chat.api_version, azure_endpoint=config.chat.azure_endpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subgraph documents: 3230\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- Prepare Subgraph Documents -------------------------\n",
    "def prepare_subgraph_documents(subgraph_text):\n",
    "    \"\"\"\n",
    "    Splits the full subgraph text into individual subgraph documents.\n",
    "    Each document is a subgraph for a specific entity.\n",
    "    \"\"\"\n",
    "    subgraph_text = subgraph_text.replace(\"\\r\", \"\").strip()\n",
    "    # Split based on the \"Subgraph for\" pattern (capturing the header/URI)\n",
    "    subgraphs = re.split(r\"\\nSubgraph for (http[s]?://[^\\s:]+):?\", subgraph_text)\n",
    "    subgraph_documents = {}\n",
    "    for i in range(1, len(subgraphs), 2):  \n",
    "        header = subgraphs[i].strip()\n",
    "        triples = subgraphs[i + 1].strip()\n",
    "        subgraph_documents[header] = triples\n",
    "    return subgraph_documents\n",
    "\n",
    "# ------------------------- Load Knowledge Graph File -------------------------\n",
    "with open(r\"kgCreation/FinSubGraphLatestPro.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    subgraph_text = f.read()\n",
    "subgraph_documents = prepare_subgraph_documents(subgraph_text)\n",
    "print(f\"Total subgraph documents: {len(subgraph_documents)}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------- Build Entity Index -------------------------\n",
    "# Create an index for entities and their corresponding subgraph headers\n",
    "entity_index = {}\n",
    "for header, doc in subgraph_documents.items():\n",
    "    lines = doc.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            obj = \" \".join(parts[2:])\n",
    "            if obj not in entity_index:\n",
    "                entity_index[obj] = []\n",
    "            entity_index[obj].append(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------- TF-IDF Ranking -------------------------\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "def tfidf_rank_documents(query_terms, documents):\n",
    "    \"\"\"\n",
    "    Ranks the documents using TF-IDF.\n",
    "    Returns a list of (doc_string, similarity_score) sorted in descending order.\n",
    "    \"\"\"\n",
    "    doc_ids = list(documents.keys())\n",
    "    docs = [f\"Subgraph for {header}\\n{documents[header]}\" for header in doc_ids]\n",
    "    tfidf_vectorizer.fit(docs)\n",
    "    query_text = \" \".join(query_terms)\n",
    "    query_vector = tfidf_vectorizer.transform([query_text])\n",
    "    doc_tfidf_matrix = tfidf_vectorizer.transform(docs)\n",
    "    scores = cosine_similarity(query_vector, doc_tfidf_matrix).flatten()\n",
    "    results = []\n",
    "    for idx, score in enumerate(scores):\n",
    "        header = doc_ids[idx]\n",
    "        doc_text = f\"Subgraph for {header}\\n{subgraph_documents[header]}\"\n",
    "        results.append((doc_text, score))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- DPR Ranking -------------------------\n",
    "# Initialize DPR models and tokenizers (using pre-trained models from Facebook)\n",
    "dpr_question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "dpr_question_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "dpr_context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "dpr_context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "def dpr_rank_documents(query_terms, documents, max_length=512):\n",
    "    \"\"\"\n",
    "    Ranks the documents using DPR.\n",
    "    Computes DPR embeddings on the fly for both the query and each document.\n",
    "    Returns a list of (doc_string, similarity_score) sorted in descending order.\n",
    "    \"\"\"\n",
    "    query_text = \" \".join(query_terms)\n",
    "    query_inputs = dpr_question_tokenizer(query_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = dpr_question_model(**query_inputs).pooler_output  \n",
    "\n",
    "    results = []\n",
    "    for header, doc in documents.items():\n",
    "        full_text = f\"Subgraph for {header}\\n{doc}\"\n",
    "        context_inputs = dpr_context_tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        with torch.no_grad():\n",
    "            doc_embedding = dpr_context_model(**context_inputs).pooler_output  \n",
    "        similarity = torch.cosine_similarity(query_embedding, doc_embedding).item()\n",
    "        results.append((full_text, similarity))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Reciprocal Rank Fusion (RRF) -------------------------\n",
    "def rrf_fusion(tfidf_results, dpr_results, k=60):\n",
    "    \"\"\"\n",
    "    Combines two ranked lists using Reciprocal Rank Fusion.\n",
    "    \"\"\"\n",
    "    doc_to_rrf_score = {}\n",
    "    # Process TF-IDF results\n",
    "    for rank, (doc, _) in enumerate(tfidf_results, start=1):\n",
    "        doc_to_rrf_score.setdefault(doc, 0)\n",
    "        doc_to_rrf_score[doc] += 1.0 / (k + rank)\n",
    "    # Process DPR results\n",
    "    for rank, (doc, _) in enumerate(dpr_results, start=1):\n",
    "        doc_to_rrf_score.setdefault(doc, 0)\n",
    "        doc_to_rrf_score[doc] += 1.0 / (k + rank)\n",
    "    fused_list = [(doc, score) for doc, score in doc_to_rrf_score.items()]\n",
    "    fused_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return fused_list\n",
    "\n",
    "def rank_documents_rrf(query_terms, documents):\n",
    "    tfidf_ranked_docs = tfidf_rank_documents(query_terms, documents)\n",
    "    dpr_ranked_docs = dpr_rank_documents(query_terms, documents)\n",
    "    return rrf_fusion(tfidf_ranked_docs, dpr_ranked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Query Parsing with LLM and Caching -------------------------\n",
    "query_cache = {} \n",
    "def parse_query_with_llm(query):\n",
    "    if query in query_cache:\n",
    "        return query_cache[query]\n",
    "\n",
    "    config = load_config()\n",
    "    llm = initialize_azure_client(config)\n",
    "    entity_extraction_prompt = [\n",
    "        {\n",
    "                        \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are given a natural language query related to employee and organization data. Extract the following details from the query:\n",
    "            - Entities mentioned in the query (e.g., Person name, Organization ID )\n",
    "            - Relationships or attributes being asked for (e.g., line manager, contact info, worksFor)\n",
    "            - Map these relationships or attributes to the correct schema predicates as defined below:\n",
    "            - Person type: Person\n",
    "            - Organization type: Organization\n",
    "            - Application type: Application\n",
    "            - Process Type: Process\n",
    "            - Works For: worksFor\n",
    "            - Email address: email\n",
    "            - last Name: familyName\n",
    "            - First Name: givenName\n",
    "            - Organisation name : name\n",
    "            - Employee status: status\n",
    "            - Description of the organisation : description\n",
    "            - Location: location\n",
    "            - Identifier (GID): gid\n",
    "            - Job Title: jobTitle\n",
    "            - Functional Manager: functionalManager\n",
    "            - Manager: hasManager\n",
    "            - Manges/reporting to: manages\n",
    "            - Contact Info: telephone\n",
    "            - User Type: userType\n",
    "            - Parent Organization: parentOrganization\n",
    "            - Has Head: hasHead\n",
    "            - Has Child Organization: hasChildOrganization\n",
    "            - Organisation has Process: hasProcess\n",
    "            - Title of the application: appName\n",
    "            - Description of the application : appDescription\n",
    "            - Application access link : accessLink \n",
    "            - Application link : appLink\n",
    "            - Application image: appImage\n",
    "            - Application belong to the organisation : partOfOrg\n",
    "            - Application managed by: managedBy\n",
    "            - people managing application : manages\n",
    "            - Application Owner: hasOwner\n",
    "            - Application part of Process: partOfProcess\n",
    "            - Process title: title\n",
    "            - Process description: description\n",
    "            - Process description: description\n",
    "            - Process has application: hasApplication\n",
    "            - Process has owner: hasOwner\n",
    "            - Process has manager: managedBy\n",
    "            - Process has child process: hasChildProcess\n",
    "            - Employee manages process: manages\n",
    "            - Process has a parent process: prentProcess\n",
    "            - Process part of an organisation: partOfOrg\n",
    "            - Process Id : processId\n",
    "            - Process reference Urls: referenceUrls\n",
    "            - Process template urls: templateUrls\n",
    "            - The goal of the query\n",
    "\n",
    "            Provide the response in a JSON format with keys: \"entities\", \"relationships\", \"goal\".\n",
    "            Do not include additional formatting other than JSON.\n",
    "            Sample Output:\n",
    "            {{\n",
    "            \"entities\": {{\n",
    "                \"Dominik Schlueter\": \"Person\",\n",
    "                \"Anubhuti Singh\": \"Person\"\n",
    "            }},\n",
    "            \"relationships\": {{\n",
    "                \"telephone\": \"phone number\",\n",
    "                \"gid\": \"gid\",\n",
    "                \"email\": \"email\"\n",
    "            }},\n",
    "            \"goal\": \"To retrieve the phone number, GID, and email address of Dominik Schlueter and Anubhuti Singh.\"\n",
    "            }}\n",
    "            Query: \"{query}\"\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=entity_extraction_prompt)\n",
    "    response_content = response.choices[0].message.content.strip()\n",
    "    parsed_data = json.loads(response_content)\n",
    "\n",
    "    entities = parsed_data.get('entities', [])\n",
    "    relationships = parsed_data.get('relationships', [])\n",
    "    goal = parsed_data.get('goal', \"\")\n",
    "\n",
    "    if isinstance(entities, dict):\n",
    "        entities = [{\"name\": k, \"type\": v} for k, v in entities.items()]\n",
    "\n",
    "    if isinstance(relationships, dict):\n",
    "        relationships = [{\"relation\": k, \"value\": v} for k, v in relationships.items()]\n",
    "\n",
    "    result = {\n",
    "        \"entities\": entities,\n",
    "        \"relationships\": relationships,\n",
    "        \"goal\": goal\n",
    "    }\n",
    "    query_cache[query] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------- Subgraph Matching and Retrieval -------------------------\n",
    "def match_subgraphs(parsed_query, subgraph_documents):\n",
    "    \"\"\"\n",
    "    Retrieves relevant subgraphs based on entities and relationships in the parsed query.\n",
    "    Uses entity filtering, ranking with TF-IDF and DPR, and additional expansion.\n",
    "    \"\"\"\n",
    "    entities = parsed_query.get(\"entities\", [])\n",
    "    relationships = parsed_query.get(\"relationships\", [])\n",
    "\n",
    "    # Extract entity names\n",
    "    entity_names = []\n",
    "    for ent in entities:\n",
    "        if isinstance(ent, dict):\n",
    "            if ent.get('name', '').lower() in ['person', 'organization', 'application', 'process']:\n",
    "                entity_names.append(ent.get('type', ''))\n",
    "            else:\n",
    "                entity_names.append(ent.get('name', ''))\n",
    "        elif isinstance(ent, str):\n",
    "            entity_names.append(ent)\n",
    "\n",
    "    # Extract relationship names and any related entity values\n",
    "    relationship_names = []\n",
    "    related_entities = []\n",
    "    for rel in relationships:\n",
    "        if isinstance(rel, dict):\n",
    "            if 'relation' in rel:\n",
    "                relationship_names.append(rel['relation'])\n",
    "            if 'predicate' in rel:\n",
    "                relationship_names.append(rel['predicate'])\n",
    "            if 'value' in rel:\n",
    "                val = rel['value']\n",
    "                if isinstance(val, str):\n",
    "                    related_entities.append(val)\n",
    "        elif isinstance(rel, str):\n",
    "            relationship_names.append(rel)\n",
    "\n",
    "    # Combine entity and relationship terms into query terms\n",
    "    query_terms = [str(term) for term in (entity_names + relationship_names)]\n",
    "\n",
    "    # Use the entity_index to filter subgraph documents quickly\n",
    "    filtered_headers = set()\n",
    "    for name in entity_names:\n",
    "        if name in entity_index:\n",
    "            filtered_headers.update(entity_index[name])\n",
    "    if filtered_headers:\n",
    "        filtered_documents_by_entity = {h: subgraph_documents[h] for h in filtered_headers}\n",
    "    else:\n",
    "        filtered_documents_by_entity = subgraph_documents\n",
    "\n",
    "    # Rank filtered documents based on the combined query terms\n",
    "    ranked_entity_matches = rank_documents_rrf(query_terms, filtered_documents_by_entity)\n",
    "\n",
    "    # Identify primary IDs by scanning documents for subjects matching any entity names\n",
    "    subjects_found = set()\n",
    "    for header, doc_str in filtered_documents_by_entity.items():\n",
    "        lines = doc_str.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                subj = parts[0]\n",
    "                obj = \" \".join(parts[2:])\n",
    "                if obj in entity_names:\n",
    "                    subjects_found.add(subj)\n",
    "                    break\n",
    "\n",
    "    primary_id_list = [str(pid) for pid in subjects_found] if subjects_found else []\n",
    "\n",
    "    if primary_id_list:\n",
    "        primary_filtered_headers = set()\n",
    "        for pid in primary_id_list:\n",
    "            if pid in entity_index:\n",
    "                primary_filtered_headers.update(entity_index[pid])\n",
    "        if primary_filtered_headers:\n",
    "            filtered_documents_by_primary_id = {h: subgraph_documents[h] for h in primary_filtered_headers}\n",
    "        else:\n",
    "            filtered_documents_by_primary_id = subgraph_documents\n",
    "    else:\n",
    "        filtered_documents_by_primary_id = subgraph_documents\n",
    "\n",
    "    ranked_primary_id_matches = rank_documents_rrf(primary_id_list, filtered_documents_by_primary_id) if primary_id_list else []\n",
    "\n",
    "    top_ranked_entity_docs = ranked_entity_matches[:15]\n",
    "    top_ranked_primary_id_docs = ranked_primary_id_matches[:15] if ranked_primary_id_matches else []\n",
    "\n",
    "    # Boost the top document in each ranked list\n",
    "    if top_ranked_entity_docs:\n",
    "        doc, score = top_ranked_entity_docs[0]\n",
    "        top_ranked_entity_docs[0] = (doc, score + 10.0)\n",
    "    if top_ranked_primary_id_docs:\n",
    "        doc, score = top_ranked_primary_id_docs[0]\n",
    "        top_ranked_primary_id_docs[0] = (doc, score + 10.0)\n",
    "\n",
    "    relationship_set = set(relationship_names)\n",
    "    additional_docs = []\n",
    "    visited_uris = set()\n",
    "\n",
    "    def retrieve_additional_docs_for_top_doc(top_docs, relationship_set, subgraph_documents, visited_uris):\n",
    "        if not top_docs:\n",
    "            return []\n",
    "        top_doc, top_score = top_docs[0]\n",
    "        local_additional_docs = []\n",
    "        lines = top_doc.split(\"\\n\")\n",
    "        for line in lines[1:]:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                subj = parts[0]\n",
    "                pred = parts[1]\n",
    "                obj = \" \".join(parts[2:])\n",
    "                for rel_name in relationship_set:\n",
    "                    if pred.endswith(rel_name) and obj in subgraph_documents:\n",
    "                        if obj not in visited_uris:\n",
    "                            visited_uris.add(obj)\n",
    "                            doc_text = f\"Subgraph for {obj}\\n{subgraph_documents[obj]}\"\n",
    "                            local_additional_docs.append((doc_text, 0.0))\n",
    "        return local_additional_docs\n",
    "\n",
    "    additional_docs.extend(retrieve_additional_docs_for_top_doc(top_ranked_entity_docs, relationship_set, subgraph_documents, visited_uris))\n",
    "    additional_docs.extend(retrieve_additional_docs_for_top_doc(top_ranked_primary_id_docs, relationship_set, subgraph_documents, visited_uris))\n",
    "\n",
    "    combined_top_docs = top_ranked_entity_docs + top_ranked_primary_id_docs + additional_docs\n",
    "    return combined_top_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Generate Answer with LLM -------------------------\n",
    "def format_answer(answer):\n",
    "    formatted_lines = []\n",
    "    for line in answer.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith(\"-\"):\n",
    "            formatted_lines.append(f\"- {line}\")\n",
    "        elif line:\n",
    "            formatted_lines.append(line)\n",
    "    return \"\\n\".join(formatted_lines)\n",
    "\n",
    "def generate_answer_with_llm(query, top_documents):\n",
    "    config = load_config()\n",
    "    llm = initialize_azure_client(config)\n",
    "    context = \"\\n\\n\".join([doc for doc, score in top_documents])\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are an AI assistant tasked with answering a query based on the provided context about employees and organizations.\n",
    "            Please provide a detailed and well-structured answer to the user's question.\n",
    "\n",
    "            - If applicable, organize the answer into bullet points.\n",
    "            - Use headings where relevant to indicate different parts of the answer.\n",
    "            - If multiple people or entities are mentioned, separate them clearly.\n",
    "            - Include all relevant details in a concise yet informative way.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: \"{query}\"\n",
    "\n",
    "            Provide a well-structured and easy-to-read answer.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = llm.chat.completions.create(model=config.chat.model, messages=prompt)\n",
    "    response_content = response.choices[0].message.content.strip()\n",
    "    return format_answer(response_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "def process_queries_and_store_subgraphs(input_excel, output_excel, subgraph_documents, k_values=[1,2, 3,5, 8, 13, 15, 21]):\n",
    "    \"\"\"\n",
    "    Reads queries from an Excel file, ranks subgraphs for each query, generates LLM responses, \n",
    "    and writes top-K subgraphs and answers for Hits@K back to a new Excel file.\n",
    "\n",
    "    Args:\n",
    "        input_excel: Path to the input Excel file with queries.\n",
    "        output_excel: Path to save the updated Excel file with responses.\n",
    "        subgraph_documents: The subgraph documents for context.\n",
    "        k_values: List of K values for Hits@K evaluation (e.g., [1, 3, 8, 13, 15, 21]).\n",
    "    \"\"\"\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_excel)\n",
    "        \n",
    "        # Ensure the required columns are present\n",
    "    if 'Query' not in df.columns or 'Ground Truth Answer' not in df.columns:\n",
    "        raise ValueError(\"Excel file must contain 'Query' and 'Ground Truth Answer' columns.\")\n",
    "\n",
    "    # Initialize columns for top-K subgraphs and LLM-generated answers\n",
    "    for k in k_values:\n",
    "        df[f'Top-{k} Subgraphs'] = ''\n",
    "        df[f'Top-{k} Answer'] = ''\n",
    "        df[f\"Top-{k} Response Time(s)\"] = ''\n",
    "        df[f\"Top-{k} Retrieval Time(s)\"] = ''\n",
    "\n",
    "    # Process each query\n",
    "    for index, row in df.iterrows():\n",
    "        query = row['Query']\n",
    "        try:\n",
    "            # Parse the query\n",
    "            start_time = time.time()\n",
    "            parsed_query = parse_query_with_llm(query)\n",
    "            df.at[index, 'Parsed Query'] = json.dumps(parsed_query)\n",
    "            # Rank subgraphs based on the parsed query\n",
    "            ranked_subgraphs = match_subgraphs(parsed_query, subgraph_documents)\n",
    "            end_time = time.time()\n",
    "            subgraph_retrieval_time = end_time - start_time\n",
    "            \n",
    "            for k in k_values:\n",
    "                # Get the top-K subgraphs\n",
    "                start_time = time.time()\n",
    "                top_k_subgraphs = ranked_subgraphs[:k]\n",
    "                response = generate_answer_with_llm(query, top_k_subgraphs)\n",
    "                end_time = time.time()\n",
    "                response_time = end_time - start_time\n",
    "                top_k_docs = [doc for doc, _ in top_k_subgraphs]\n",
    "\n",
    "                # Store the subgraphs as a string\n",
    "                df.at[index, f'Top-{k} Subgraphs'] = \"\\n\\n\".join(top_k_docs)\n",
    "                # Generate the answer using the top-K subgraphs\n",
    "                \n",
    "                df.at[index, f'Top-{k} Answer'] = response\n",
    "                df.at[index, f\"Top-{k} Retrieval Time(s)\"] = subgraph_retrieval_time\n",
    "                df.at[index, f\"Top-{k} Response Time(s)\"] = response_time\n",
    "\n",
    "        except Exception as e:\n",
    "            df.at[index, 'Parsed Query'] = f\"Error: {e}\"\n",
    "            # Log any errors during processing\n",
    "            for k in k_values:\n",
    "                df.at[index, f'Top-{k} Subgraphs'] = f\"Error: {e}\"\n",
    "                df.at[index, f'Top-{k} Answer'] = f\"Error: {e}\"\n",
    "                df.at[index, f\"Top-{k} Retrieval Time(s)\"] = None\n",
    "                df.at[index, f\"Top-{k} Response Time(s)\"] = None\n",
    "\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Updated Excel file saved to {output_excel}\")\n",
    "        \n",
    "# ------------------------- Usage -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_excel_path = r\"data/LLMEval_1.xlsx\"\n",
    "    output_excel_path = r\"Outputs/LLM_responses_rag_subgraphs_multiHop.xlsx\"\n",
    "\n",
    "    # Call the function to process queries and store results\n",
    "    process_queries_and_store_subgraphs(input_excel_path, output_excel_path, subgraph_documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
